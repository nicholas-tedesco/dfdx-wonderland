{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dd82887",
   "metadata": {},
   "source": [
    "# **Differentiable Wonderland: Chapter 4**\n",
    "goal: implement solutions to chapter 4 problems from Alice's Adventures in a Differentiable Wonderland, with particular focus given to...\n",
    "\n",
    "1) linear models \n",
    "2) optimization via gradient descent \n",
    "3) train-test splitting and model comparison "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c59049c",
   "metadata": {},
   "source": [
    "## **Packages and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba682b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, utils\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier \n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "5f4ad389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69651166",
   "metadata": {},
   "source": [
    "## **Practice Problems**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db175388",
   "metadata": {},
   "source": [
    "#### Problem 1\n",
    "\n",
    "Load a toy dataset: for example, one of those contained in the scikit-learn datasets module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59671037",
   "metadata": {},
   "source": [
    "$\\underline{\\text{SOLUTION}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "5031cd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded classification dataset.\n",
      "- Rows: 569\n",
      "- Cols: 30\n",
      "Successfully loaded regression data.\n",
      "- Rows: 442\n",
      "- Cols: 10\n"
     ]
    }
   ],
   "source": [
    "## dataset for classification - Wisconsin breast cancer \n",
    "clf_data = datasets.load_breast_cancer()\n",
    "print('Successfully loaded classification dataset.')\n",
    "print(f'- Rows: {clf_data.data.shape[0]:,}')\n",
    "print(f'- Cols: {clf_data.data.shape[1]:,}')\n",
    "\n",
    "## dataset for regression - diabetes \n",
    "reg_data = datasets.load_diabetes()\n",
    "print('Successfully loaded regression data.')\n",
    "print(f'- Rows: {reg_data.data.shape[0]:,}')\n",
    "print(f'- Cols: {reg_data.data.shape[1]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e4eac7",
   "metadata": {},
   "source": [
    "Let's also split our data into training and test sets, which will be useful for later problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "788f6b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset:utils._bunch.Bunch, train_prop:float, random_seed:int) -> tuple[np.ndarray]: \n",
    "\n",
    "    \"\"\" given input data and a training ratio, \n",
    "        randomly splits data into training and test subsets.  \n",
    "    \"\"\"\n",
    "\n",
    "    ## select training vs. test indices \n",
    "    np.random.seed(random_seed) \n",
    "    indices = np.arange(dataset.data.shape[0])\n",
    "    \n",
    "    n_train = np.ceil(dataset.data.shape[0] * train_prop).astype(int) \n",
    "    train_indices = np.random.choice(indices, n_train)\n",
    "    test_indices = indices[~(np.isin(indices, train_indices))]\n",
    "\n",
    "    ## create training and test partitions \n",
    "    X_train = dataset.data[train_indices] \n",
    "    X_test  = dataset.data[test_indices] \n",
    "\n",
    "    y_train = dataset.target[train_indices]\n",
    "    y_test  = dataset.target[test_indices] \n",
    "\n",
    "    return (X_train, X_test, y_train, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "4abdf0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished splitting classification data.\n",
      "- Train Size: 456\n",
      "- Test Size:  252\n",
      "Finished splitting regression data.\n",
      "- Train Size: 354\n",
      "- Test Size:  197\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PROP = 0.80\n",
    "RANDOM_STATE = 15213\n",
    "\n",
    "## split classification data \n",
    "clf_X_train, clf_X_test, clf_y_train, clf_y_test = split_data(clf_data, TRAIN_PROP, RANDOM_STATE)\n",
    "print('Finished splitting classification data.') \n",
    "print(f'- Train Size: {clf_X_train.shape[0]:,}')\n",
    "print(f'- Test Size:  {clf_X_test.shape[0]:,}')\n",
    "\n",
    "## split regression data \n",
    "reg_X_train, reg_X_test, reg_y_train, reg_y_test = split_data(reg_data, TRAIN_PROP, RANDOM_STATE)\n",
    "print('Finished splitting regression data.') \n",
    "print(f'- Train Size: {reg_X_train.shape[0]:,}')\n",
    "print(f'- Test Size:  {reg_X_test.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3bf63b",
   "metadata": {},
   "source": [
    "#### Problem 2\n",
    "\n",
    "Build a linear model (for regression or classification depending on the dataset). Think about how to make the code as modular as possible: as we will see, you will need at least two functions, one for initializing the parameters of the model and one for computing the model's predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f3a4ac",
   "metadata": {},
   "source": [
    "$\\underline{\\text{SOLUTION}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "16d21b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel: \n",
    "\n",
    "    \"\"\" interface class used for linear models. \n",
    "        provides methods for construction and model training. use must implement method for prediction. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X:np.ndarray, y:np.ndarray, bias:bool): \n",
    "\n",
    "        ## check input validity \n",
    "        try: \n",
    "            assert (len(X.shape) == 2)\n",
    "            assert (len(y.shape) == 1)\n",
    "        except: \n",
    "            raise Exception('X must be 2-dimensional numpy array; y must be 1-dimensional numpy array.')\n",
    "\n",
    "        ## initialize class members \n",
    "        self.bias = bias \n",
    "        self.n = X.shape[0] \n",
    "        self.p = X.shape[1] + int(bias) \n",
    "\n",
    "        self.X = self.calculate_design_matrix(X) \n",
    "        self.y = y \n",
    "\n",
    "        self.w = np.random.random(self.p) \n",
    "        self.grad = np.zeros((self.p,)) \n",
    "\n",
    "\n",
    "    def calculate_design_matrix(self, X:np.ndarray): \n",
    "\n",
    "        \"\"\" translates the input data into a design matrix; only modifies input data if bias is specified.\n",
    "        \"\"\"\n",
    "\n",
    "        design_mat = X\n",
    "        if (self.bias): \n",
    "            design_mat = np.concat([X, np.ones(X.shape[0]).reshape(X.shape[0], 1)], axis=1) \n",
    "        \n",
    "        return design_mat \n",
    "    \n",
    "\n",
    "    def calculate_gradient(self): \n",
    "\n",
    "        \"\"\" calculates gradient, which is specific to subclass implementation. \n",
    "        \"\"\"\n",
    "\n",
    "        pass \n",
    "\n",
    "\n",
    "    def train(self, n_iter:int, lr:int, momentum:int, verbose:bool=True) -> None: \n",
    "\n",
    "        \"\"\" optimizes model weights using gradient descent. \n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(n_iter): \n",
    "\n",
    "            ## calculate gradient over entire dataset \n",
    "            grad = self.calculate_gradient() \n",
    "            self.grad = grad + momentum * self.grad\n",
    "\n",
    "            ## perform weight update step \n",
    "            self.w = self.w - lr * self.grad \n",
    "\n",
    "        print(f'Finished Model Training. Norm of Final Gradient: {grad.T @ grad:.4f}')\n",
    "            \n",
    "\n",
    "    def predict(self, X:np.ndarray) -> np.ndarray: \n",
    "\n",
    "        \"\"\" calculates predictions for input matrix using linear / logistic regression with weight vector. \n",
    "        \"\"\"\n",
    "\n",
    "        pass \n",
    "\n",
    "\n",
    "class LinearRegressor(LinearModel): \n",
    "\n",
    "    \"\"\" class to create, train, and predict with regards to a linear regression model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X:np.ndarray, y:np.ndarray, bias:bool=True): \n",
    "        super(LinearRegressor, self).__init__(X, y, bias)\n",
    "\n",
    "    \n",
    "    def calculate_gradient(self): \n",
    "\n",
    "\n",
    "        \"\"\" calculates gradient for linear regression model.\n",
    "        \"\"\"\n",
    "\n",
    "        return (-2 / self.n) * self.X.T @ (self.y - self.X @ self.w) \n",
    "\n",
    "\n",
    "    def predict(self, X:np.ndarray): \n",
    "\n",
    "        \"\"\" given a group of instances, \n",
    "            computes predictions via linear regression + the current weights. \n",
    "        \"\"\"\n",
    "\n",
    "        ## validate input \n",
    "        try: \n",
    "            assert (len(X.shape) == 2)\n",
    "        except: \n",
    "            raise Exception('input to predict must be 2-dimensional numpy array.')\n",
    "\n",
    "        ## convert test input into design matrix \n",
    "        X = self.calculate_design_matrix(X) \n",
    "\n",
    "        ## calculate predictions\n",
    "        y_hat = X @ self.w\n",
    "\n",
    "        return y_hat\n",
    "    \n",
    "\n",
    "class BinaryLinearClassifier(LinearModel): \n",
    "\n",
    "    \"\"\" class to create, train, and predict with regards to a binary logistic regression model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X:np.ndarray, y:np.ndarray, bias:bool=True): \n",
    "        super(BinaryLinearClassifier, self).__init__(X, y, bias) \n",
    "        self.vectorized_sigmoid = np.vectorize(self.scalar_sigmoid)\n",
    "\n",
    "\n",
    "    def scalar_sigmoid(self, x:float): \n",
    "\n",
    "        \"\"\" computes sigmoid for a scalar input. \n",
    "        \"\"\"\n",
    "\n",
    "        return 1 / (1 + np.exp(-x)) \n",
    "    \n",
    "\n",
    "    def calculate_gradient(self):\n",
    "        \n",
    "        \"\"\" calculates gradient for binary logistic regression model.\n",
    "        \"\"\"\n",
    "\n",
    "        return (1 / self.n) * self.X.T @ (self.vectorized_sigmoid(self.X @ self.w) - self.y) \n",
    "    \n",
    "\n",
    "    def predict(self, X:np.ndarray): \n",
    "\n",
    "        \"\"\" given a group of instances, \n",
    "            computes predictions via binary logistic regression + the current weights. \n",
    "        \"\"\"\n",
    "        \n",
    "        ## validate input \n",
    "        try: \n",
    "            assert (len(X.shape) == 2)\n",
    "        except: \n",
    "            raise Exception('input to predict must be 2-dimensional numpy array.') \n",
    "        \n",
    "        ## convert test input into design matrix \n",
    "        X = self.calculate_design_matrix(X) \n",
    "\n",
    "        ## caculate predictions \n",
    "        logits = X @ self.w \n",
    "        y_hat = self.vectorized_sigmoid(logits) \n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a8aec9",
   "metadata": {},
   "source": [
    "#### Problem 3\n",
    "\n",
    "Train the model via gradient descent. For now you can compute the gradients maually: try to imagine how you can make also this part modulear, i.e., how do you change the gradient's computation if you want to dynamically add or remove the bias from a model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf3b2da",
   "metadata": {},
   "source": [
    "$\\underline{\\text{SOLUTION}}$\n",
    "\n",
    "(note that all code for gradient descent is implemented in the previous problem)\n",
    "\n",
    "Let's start by deriving our gradient for a linear regression model, with a mean squared error loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44634d3",
   "metadata": {},
   "source": [
    "$$\\nabla_w L(w) = \\nabla_w \\frac{1}{n} \\Sigma_i (y_i - w^Tx_i)^2$$\n",
    "$$\\nabla_w L(w) = \\nabla_w \\frac{1}{n} \\Sigma_i (y_i^2 - 2yw^Tx_i + (w^Tx_i)^2)$$\n",
    "$$\\nabla_w L(w) = \\frac{1}{n} \\Sigma_i (-2yx_i + w^Tx_i^2)$$\n",
    "$$\\nabla_w L(w) = \\frac{-2}{n} \\Sigma_i x_i(y - w^Tx_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0dd983",
   "metadata": {},
   "source": [
    "Next, we'll derive the gradient for a logistic regression model, with a binary cross entropy loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f5a80",
   "metadata": {},
   "source": [
    "$$\\nabla_w L(w) = \\nabla_w -\\frac{1}{n} \\Sigma_i y_i * -\\log[\\hat{p_i}] + (1 - y_i) * -\\log[1 - \\hat{p_i}]$$\n",
    "$$\\hat{p} = \\sigma(z) = \\frac{1}{1 + e^{-z}}, ~~ z = Xw$$\n",
    "$$\\frac{\\partial{L}}{\\partial{w}} = \\frac{\\partial{L}}{\\partial{y}} \\cdot \\frac{\\partial{y}}{\\partial{z}} \\cdot \\frac{\\partial{z}}{\\partial{w}}$$\n",
    "$$\\frac{\\partial{L}}{\\partial{\\hat{p}}} = -\\frac{y}{\\hat{p}} + \\frac{1 - y}{1 - \\hat{p}} = \\frac{\\hat{p} - y}{\\hat{p}(1 - \\hat{p})}, ~~ \\frac{\\partial{\\hat{p}}}{\\partial{z}} = \\hat{p}(1 - \\hat{p}), ~~ \\frac{\\partial{z}}{\\partial{w}} = X$$\n",
    "$$\\frac{\\partial{L}}{\\partial{w}} = \\frac{\\hat{p} - y}{\\hat{p}(1 - \\hat{p})} \\cdot \\hat{p}(1 - \\hat{p}) \\cdot X$$\n",
    "$$\\frac{\\partial{L}}{\\partial{w}} = X(\\hat{p} - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "a9333b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Model Training. Norm of Final Gradient: 0.0035\n"
     ]
    }
   ],
   "source": [
    "## train regression model \n",
    "reg_mod = LinearRegressor(reg_X_train, reg_y_train, bias=True)\n",
    "reg_mod.train(n_iter=50_000, lr=0.1, momentum=0.4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "0ecdab54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Model Training. Norm of Final Gradient: 9071.1961\n"
     ]
    }
   ],
   "source": [
    "## train classification model\n",
    "clf_mod = BinaryLinearClassifier(clf_X_train, clf_y_train, bias=True) \n",
    "clf_mod.train(n_iter=100_000, lr=0.01, momentum=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3110d1d0",
   "metadata": {},
   "source": [
    "#### Problem 4\n",
    "\n",
    "Plot the loss function and the accuracy on an independent test set. If you know some standard machine learning, you can compare the results to other supervised learning models, such as a decision tree or a k-NN model. Use scikit-learn for the other models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b1598",
   "metadata": {},
   "source": [
    "$\\underline{\\text{SOLUTION}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d87ddf",
   "metadata": {},
   "source": [
    "First, let's create a few functions for our scoring metrics, then apply them to our existing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "39ae61eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(y_true:np.ndarray, y_pred:np.ndarray) -> float: \n",
    "\n",
    "    \"\"\" given a vector of true labels and a vector of predictions, \n",
    "        calculates + returns accuracy score  \n",
    "    \"\"\"\n",
    "\n",
    "    ## validate input \n",
    "    try: \n",
    "        assert (len(y_true.shape) == 1)\n",
    "        assert (len(y_pred.shape) == 1) \n",
    "    except: \n",
    "        raise Exception('input vectors must be 1-dimensional.')\n",
    "    \n",
    "    try: \n",
    "        assert (y_true.shape[0] == y_pred.shape[0]) \n",
    "    except: \n",
    "        raise Exception('input vectors must be the same length.') \n",
    "    \n",
    "    ## calculate accuracy \n",
    "    n_total = y_true.shape[0] \n",
    "    n_correct = y_true[y_true == y_pred].shape[0]\n",
    "\n",
    "    return n_correct / n_total \n",
    "\n",
    "\n",
    "def calc_mean_squared_error(y_true:np.ndarray, y_pred:np.ndarray) -> float: \n",
    "\n",
    "    \"\"\" given a vector of true labels and a vector of predictions, \n",
    "        calculates + returns mean squared error.  \n",
    "    \"\"\"\n",
    "\n",
    "    ## validate input \n",
    "    try: \n",
    "        assert (len(y_true.shape) == 1)\n",
    "        assert (len(y_pred.shape) == 1) \n",
    "    except: \n",
    "        raise Exception('input vectors must be 1-dimensional.')\n",
    "    \n",
    "    try: \n",
    "        assert (y_true.shape[0] == y_pred.shape[0]) \n",
    "    except: \n",
    "        raise Exception('input vectors must be the same length.') \n",
    "    \n",
    "    ## calculate mean squared error \n",
    "    n_total = y_true.shape[0] \n",
    "    squared_resid = y_true - y_pred\n",
    "\n",
    "    return squared_resid.T @ squared_resid / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "1a389321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg Test MSE: 2940.4807\n",
      "Clf Test Acc: 0.8810\n"
     ]
    }
   ],
   "source": [
    "## regression \n",
    "reg_pred = reg_mod.predict(reg_X_test) \n",
    "reg_mse  = calc_mean_squared_error(reg_y_test, reg_pred) \n",
    "print(f'Reg Test MSE: {reg_mse:.04f}')\n",
    "\n",
    "## classification \n",
    "clf_pred = clf_mod.predict(clf_X_test) \n",
    "clf_acc = calc_accuracy(clf_y_test, clf_pred) \n",
    "print(f'Clf Test Acc: {clf_acc:.04f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3551ae4a",
   "metadata": {},
   "source": [
    "Now let's compare these scores to models trained via scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "60a35fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sklearn_regressor(mod, X_train, X_test, y_train, y_test) -> float: \n",
    "\n",
    "    \"\"\" given a sklearn regression model and training / test data, \n",
    "        trains the model then evaluates predictions made on the test set.  \n",
    "    \"\"\"\n",
    "\n",
    "    mod.fit(X_train, y_train) \n",
    "    pred = mod.predict(X_test) \n",
    "\n",
    "    score = calc_mean_squared_error(y_test, pred) \n",
    "    return score \n",
    "\n",
    "\n",
    "def eval_sklearn_classifier(mod, X_train, X_test, y_train, y_test) -> float: \n",
    "\n",
    "    \"\"\" given a sklearn classification model and training / test data, \n",
    "        trains the model then evaluates predictions made on the test set.  \n",
    "    \"\"\"\n",
    "\n",
    "    mod.fit(X_train, y_train) \n",
    "    pred = mod.predict(X_test) \n",
    "\n",
    "    score = calc_accuracy(y_test, pred) \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "fccf7858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-Learn Regression Models: Test Set Performance\n",
      "- Linear Reg:        2975.9868\n",
      "- K-Neighbors:       3930.4199\n",
      "- Decision Tree:     6030.0254\n",
      "\n",
      "Scikit-Learn Classification Models: Test Set Performance\n",
      "- Logistic Reg:  0.0051\n",
      "- K-Neighbors:   0.0102\n",
      "- Decision Tree: 0.0051\n"
     ]
    }
   ],
   "source": [
    "# regression -------------------------------------------\n",
    "\n",
    "print(\"Scikit-Learn Regression Models: Test Set Performance\") \n",
    "\n",
    "## linear regression \n",
    "sk_linreg_score = eval_sklearn_regressor(LinearRegression(), reg_X_train, reg_X_test, reg_y_train, reg_y_test)\n",
    "print(f'- Linear Reg:        {sk_linreg_score:.04f}')\n",
    "\n",
    "## k-neighbors regression \n",
    "sk_knnreg_score = eval_sklearn_regressor(KNeighborsRegressor(), reg_X_train, reg_X_test, reg_y_train, reg_y_test)\n",
    "print(f'- K-Neighbors:       {sk_knnreg_score:.04f}')\n",
    "\n",
    "## decision tree regression \n",
    "sk_dtreg_score = eval_sklearn_regressor(DecisionTreeRegressor(), reg_X_train, reg_X_test, reg_y_train, reg_y_test)\n",
    "print(f'- Decision Tree:     {sk_dtreg_score:.04f}')\n",
    "\n",
    "\n",
    "# classification ---------------------------------------\n",
    "\n",
    "print(\"\\nScikit-Learn Classification Models: Test Set Performance\")\n",
    "\n",
    "## logistic regression \n",
    "sk_logclf_score = eval_sklearn_classifier(LogisticRegression(), reg_X_train, reg_X_test, reg_y_train, reg_y_test)\n",
    "print(f'- Logistic Reg:  {sk_logclf_score:.04f}')\n",
    "\n",
    "## k-neighbors classifier \n",
    "sk_knnclf_score = eval_sklearn_classifier(KNeighborsClassifier(), reg_X_train, reg_X_test, reg_y_train, reg_y_test)\n",
    "print(f'- K-Neighbors:   {sk_knnclf_score:.04f}')\n",
    "\n",
    "## decision tree classifier \n",
    "sk_dtclf_score = eval_sklearn_classifier(DecisionTreeClassifier(), reg_X_train, reg_X_test, reg_y_train, reg_y_test)\n",
    "print(f'- Decision Tree: {sk_dtclf_score:.04f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfdx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
